# -*- coding: utf-8 -*-
"""pytorch fundamentals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GV0LQJc73FZEi3kdnuXRqQeJmTq-7OAn
"""

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
print(torch.__version__)

"""## Introduction to tensor"""

scalar = torch.tensor(7)
scalar

scalar.ndim

scalar.item()

#Vector
vector = torch.tensor([7,7])
vector

vector.ndim

vector.shape

matrix = torch.tensor([[7,8],[9,10]])
matrix

matrix.ndim

matrix.shape

matrix[1]

#Tensor
TENSOR = torch.tensor([[[1,2,3],[3,6,9],[2,4,5]]])
TENSOR

TENSOR.ndim

TENSOR.shape

TENSOR[0]

TENSOR_5 = torch.tensor([[[[1,2,3],[4,5,6],[7,8,9],[0,1,2]]]])
TENSOR_5

TENSOR_5.ndim

TENSOR_5.shape

"""## RANDOM TENSORS

"""

#Create a random tensors of size (3,4)
random_tensor = torch.rand(3,4)
random_tensor

random_tensor.ndim

#create r
random_tensor_size_tensor = torch.rand(size=(3,224,224))  #color channels, Height & width
random_tensor_size_tensor.shape, random_tensor_size_tensor.ndim

vid_tensor = torch.rand(size=(3,3,3))
vid_tensor.shape, vid_tensor.ndim

zeros = torch.zeros(size=(3,4))
zeros

zeros*random_tensor

ones = torch.ones(size=(3,4))
ones

ones.dtype

random_tensor.dtype

one_to_ten = torch.arange(start=1, end=11, step=1)
one_to_ten

torch.__version__

ten_zeros = torch.zeros_like(input=one_to_ten)
ten_zeros

float_32_tensor = torch.tensor([3.0,6.0,9.0],
                               dtype=None,
                               device=None,
                               requires_grad=False)
float_32_tensor

float_32_tensor.dtype

float_16_tensor = float_32_tensor.type(torch.float16)
float_16_tensor

float_16_tensor*float_32_tensor

int_32_tensor  = torch.tensor([3, 6, 9], dtype=torch.long)
int_32_tensor

float_32_tensor*int_32_tensor

some_tensor = torch.rand(3, 4)
some_tensor

some_tensor.size(), some_tensor.shape

print(some_tensor)
print(f"Datatype of tensor: {some_tensor.dtype}")
print(f"Shape of tensor: {some_tensor.shape}")
print(f"Device tensor is on: {some_tensor.device}")

tensor = torch.tensor([1,2,3])
tensor+100

tensor * 10

tensor

tensor - 10

torch.mul(tensor,10)

torch.add(tensor, 10)

tensor

tensor*tensor

torch.matmul(tensor, tensor)

tensor

1*1 + 2*2 + 3*3

# Commented out IPython magic to ensure Python compatibility.
# %%time
# value = 0
# for i in range(len(tensor)):
#   value += tensor[i]*tensor[i]
# value

# Commented out IPython magic to ensure Python compatibility.
# %%time
# torch.matmul(tensor, tensor) #executed in microseconds

tensor @ tensor

tensor & tensor

tensor_A = torch.tensor([[1,2],
                         [3,4],
                         [5,6]])
tensor_B  = torch.tensor([[7,10],
                          [8,11],
                          [9,12]])
#torch.mm(tensor_A, tensor_B)

tensor_A.shape, tensor_B.shape

tensor_B.T, tensor_B.T.shape

torch.matmul(tensor_B, tensor_B.T)

x = torch.arange(0,100, 10)
x, x.dtype

torch.max(x)

torch.min(x)

torch.mean(x.type(torch.float32)), x.type(torch.float32).mean()

torch.sum(x), x.sum()

x

x.argmin()

x[0]

x.argmax()

x[9]

##Reshaping
import torch
x = torch.arange(1., 10.)
x, x.shape

x_reshaped = x.reshape(1, 9)
x_reshaped, x_reshaped.shape

#view
z = x.view(1,9)
z, z.shape

z[:, 0] = 5
z, x

#Stack
x_stacked = torch.stack([x, x, x, x], dim=0)
x_stacked

x_reshaped

x_reshaped.squeeze()

x_reshaped.squeeze().shape

x_squeezed = x_reshaped.squeeze()
x_squeezed, x_squeezed.shape

x_unsqueezed = x_squeezed.unsqueeze(dim=0)

x_unsqueezed, x_unsqueezed.shape

